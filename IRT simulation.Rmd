---
title: "Untitled"
author: "Ben Attaway"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE)

library(mirt)
library(mice)
library(tidyverse)
library(gt)
library(SimDesign)

set.seed(1701)
```

NAEP typically aims for a discrimination parameter `a` between 0 and 2 and difficulty `b` between -3 and 3. Guessing probability `c` naturally needs to be between 0 and 1, but realistically we'd expect it to be on the low end of that range (for a five-option question I'd expect 0.2). (https://nces.ed.gov/nationsreportcard/tdw/analysis/scaling_irt.aspx)

Prof. Verkuilen suggested fixing the total test length at 30 or 40 items (so the item bank needs to be longer for a shorter anchor length), and the number of students at 1000 (didn't say per version or total, so I'll do total for now)

Try with sirt, ltm

```{r}
# Adapted from https://philchalmers.github.io/SimDesign/html/04-Auxillary_parameter_recovery.html
Design <- createDesign(n_anchor = c(3, 5, 10, 15, 30), # setting the anchor to the same number as test items should mean everyone completes everything
                       n_test_questions = c(30),
                       anchor_type = c("midi", "random", "easy"),
                       assumption = c("Equivalent groups", "NEAT"),
                       n_test_versions = c(4),
                       type = c('2PL'), # Also try 3PL
                       groupdiff = c(0, 0.4), # Thought it would be good to see how well we could assess the size of a mean difference
                       plausible_draws = c(10), # PISA provides 10 plausible values, NAEP provides 20, so those seem like good #s to pick
                       n_students_total = 1000)

make_missing <- function(full_responses, n_anchor, n_test_questions, n_groups) {
  if (n_anchor == n_test_questions) {
    return(full_responses)
  }
  else {
    responses_with_missing <- matrix(nrow=nrow(full_responses), ncol=ncol(full_responses))
    colnames(responses_with_missing) <- colnames(full_responses)
    responses_with_missing[,1:n_anchor] <- full_responses[,1:n_anchor]
    full_responses <- full_responses[, (n_anchor+1):ncol(full_responses)]
    for (i in 0:(n_groups - 1)) {
      row_range <- (1 + i*nrow(full_responses)/n_groups):((i+1)*nrow(full_responses)/n_groups)
      col_range <- (1+i*(n_test_questions-n_anchor)):((i+1)*(n_test_questions-n_anchor))
      responses_with_missing[row_range, n_anchor+col_range] <- full_responses[row_range, col_range]
    }
    return(responses_with_missing)
  }
}

reorder_anchor <- function(params, n_anchor, anchor_type) {
  if (anchor_type == "midi") {
    # Medium difficulty is b = 0, so select b values closest to 0
    # High discrimination items would obviously make the best anchor, but someone writing a test can't really tell what the discrimination
    # parameter will be, whereas difficulty is something they can plausibly guess at based on their knowledge of the subject.
    params_reordered <- params |>
      rownames_to_column() |>
      arrange(abs(b)) 
  } else if (anchor_type=="random") {
    # Parameters are already random draws from the distribution, so no need to shuffle them further
    params_reordered <- params |>
      rownames_to_column()
  } else if (anchor_type=="easy") {
    # Items with lowest b
    params_reordered <- params |>
      rownames_to_column() |>
      arrange(b)
  }
  anchor_values <- params_reordered |>
    slice_head(n=n_anchor) |>
    pull(rowname) |>
    as.numeric()
  if (n_anchor > 0) {
    params <- rbind(params[anchor_values,], params[-anchor_values,])
    # put the other values back in random order
  }
  params
}

Generate <- function(condition, fixed_objects = NULL) {
    Attach(condition) # attaches condition names for direct reference
    
    n_items = n_anchor + n_test_versions*(n_test_questions-n_anchor)

    a_values <- matrix(rlnorm(n_items, 0, .25))
    b_values <- rnorm(n_items, 0, 1.15)

    params <- data.frame()
    for(i in 1:n_items) {
       params <- rbind(params, traditional2mirt(c('a'=a_values[i], 'b'=b_values[i], 'g'=ifelse(type=='2PL', 0, .2), 'u'=1),
                                 cls=type))
    }
    colnames(params) <- c("a", "d", "g", "u")
    params$b <- b_values
        
    # Wanted to make two groups of students with different ability distributions to look at power to detect differences based on plausible values (which we can get after fitting a model)
    Theta1 <- matrix(scale(rnorm(n_students_total/2)))
    Theta2 <- matrix(scale(rnorm(n_students_total/2)) + groupdiff)

    params <- reorder_anchor(params, n_anchor, anchor_type)
    
    dat1 <- simdata(a=params$a, d=params$d, guess=params$g, Theta=Theta1, itemtype = type) 
    dat2 <- simdata(a=params$a, d=params$d, guess=params$g, Theta=Theta2, itemtype = type)
    Thetas <- rbind(Theta1, Theta2)
    dat <- rbind(dat1, dat2)
    covdata <- data.frame(group = c(rep("Group 1", n_students_total/2), rep("Group 2", n_students_total/2)))    
        
    # if (assumption != "NEAT") {
    #   # Shuffle rows so that people from groups 1 and 2 are equally likely to get any version
    #   Thetas <- cbind(dat, data.frame(Theta = c(Theta1, Theta2), group=covdata$group))
    #   Thetas <- Thetas[sample(nrow(Thetas)),]
    #   dat <- Thetas[,1:ncol(dat)]
    #   rownames(dat) <- NULL
    #   covdata$group <- Thetas$group
    #   Thetas <- Thetas$Theta
    # }
    
    dat <- as.matrix(dat) |> make_missing(n_anchor, n_test_questions, n_test_versions)

    ret <- list(dat=dat, covdata=covdata, parameters=list(Thetas=Thetas, a=params$a, b=params$b, intercept=params$d, g=params$g, groupdiff=groupdiff))
    ret
}

Analyse <- function(condition, dat, fixed_objects = NULL) {
    Attach(condition)
    # extract
    data <- dat$dat
    covdata <- dat$covdata
    parameters <- dat$parameters

    if(assumption != "NEAT") {
      mod <- manageWarnings(mirt(data, 1, itemtype=type, SE=T, dentype="Gaussian"), warning2error = "Could not invert information matrix; model may not be (empirically) identified.")
    } else {
    mod <- manageWarnings(multipleGroup(data, 1, group = covdata$group, SE=T, invariance=c('free_means', 'slopes', 'intercepts'), itemtype=type), warning2error = "Could not invert information matrix; model may not be (empirically) identified.")
    }
    if(!extract.mirt(mod, 'converged')) {
      print("Did not converge")
      stop('mirt did not converge')
    } else {
      print("Fit IRT model")
    }
    
    pred_theta <- fscores(mod, full.scores=TRUE, plausible.draws = plausible_draws)
    
    pred_theta_point <- fscores(mod, full.scores=TRUE, full.scores.SE = TRUE)
    diff_Theta <- unname(c(pred_theta_point[,1])) - parameters$Thetas
    se_Theta <- pred_theta_point[,2]
    
    pvmods <- lapply(pred_theta, function(x, covdata) lm(x ~ covdata$group), covdata=covdata)
    so <- lapply(pvmods, summary)
    par <- lapply(so, function(x) x$coefficients[, 'Estimate'])
    SEpar <- lapply(so, function(x) x$coefficients[, 'Std. Error'])
    pooled <- averageMI(par, SEpar)
    
    model_params <- coef(mod, printSE=T, IRTpars=T)
    # Don't need u estimates since not testing 4PL model
    model_params_df <- data.frame(matrix(unlist(model_params)[1:(8*ncol(data))], byrow=T, ncol=8))[,c(1:6)]
    colnames(model_params_df) <- c("a_est", "a_SE", "b_est", "b_SE", "g_est", "g_SE")
    diff_a = model_params_df$a_est - parameters$a
    diff_b = model_params_df$b_est - parameters$b
    if(max(abs(model_params_df$b_SE)) > 10) {
      stop("Item parameters poorly estimated")
    }
    diff_g = ifelse(type=="2PL", rep(0, length(parameters$d)), model_params_df$g_est - parameters$g)

    # return list
    ret <- list(true_parameters=parameters, par_coef = pooled[2,"par"], par_SE = pooled[2, "SEpar"],
                diff_Theta = diff_Theta, pred_theta=pred_theta_point, se_Theta = se_Theta,
                model_params_df = model_params_df,
                diff_a = diff_a, diff_b = diff_b,
                diff_g = diff_g, groupdiff = pooled[2,"par"])
    ret
}
Summarise <- function(condition, results, fixed_objects = NULL) {
  print("summarize")
  index <- 1:length(results)
  # for ease of calculations, find the difference between the observed and population matrices
  bias_Theta <- bias(do.call(c, lapply(index, function(ind, res) res[[ind]]$diff_Theta,
                   res=results)))
  RMSE_Theta <- RMSE(do.call(c, lapply(index, function(ind, res) res[[ind]]$diff_Theta,
                   res=results)))
  SE_Theta <- mean(do.call(c, lapply(index, function(ind, res) res[[ind]]$se_Theta,
                   res=results)))

  bias_groupdiff <- bias(do.call(c, lapply(index, function(ind, res) res[[ind]]$groupdiff - res[[ind]]$true_parameters$groupdiff,
                   res=results)))
  RMSE_groupdiff <- RMSE(do.call(c, lapply(index, function(ind, res) res[[ind]]$groupdiff - res[[ind]]$true_parameters$groupdiff,
                   res=results)))

  SE_groupdiff <- mean(do.call(c, lapply(index, function(ind, res) res[[ind]]$par_SE,
                   res=results)))
  
  bias_a <- bias(do.call(c, lapply(index, function(ind, res) res[[ind]]$diff_a,
                   res=results)))
  RMSE_a <- RMSE(do.call(c, lapply(index, function(ind, res) res[[ind]]$diff_a,
                   res=results)))
  SE_a <- mean(do.call(c, lapply(index, function(ind, res) res[[ind]]$model_params_df$a_SE,
                    res=results)))

  bias_b <- bias(do.call(c, lapply(index, function(ind, res) res[[ind]]$diff_b,
                   res=results)))
  RMSE_b <- RMSE(do.call(c, lapply(index, function(ind, res) res[[ind]]$diff_b,
                   res=results)))
  SE_b <- mean(do.call(c, lapply(index, function(ind, res) res[[ind]]$model_params_df$b_SE,
                    res=results)))

  bias_g <- bias(do.call(c, lapply(index, function(ind, res) res[[ind]]$diff_g,
                   res=results)))
  RMSE_g <- RMSE(do.call(c, lapply(index, function(ind, res) res[[ind]]$diff_g,
                   res=results)))
  SE_g <- mean(do.call(c, lapply(index, function(ind, res) res[[ind]]$model_params_df$g_SE,
                    res=results)))
  
  # I'm more interested in predicting ability (theta) but a lot of papers do focus more on parameter retrieval so I would like to at
  # least report the mean and RMSE for those as well. Don't have time for that tonight so TODO
  ret <- c(bias_Theta=bias_Theta, RMSE_Theta = RMSE_Theta, SE_Theta=SE_Theta, bias_groupdiff=bias_groupdiff,
           RMSE_groupdiff = RMSE_groupdiff, SE_groupdiff = SE_groupdiff,
           bias_a = bias_a, RMSE_a = RMSE_a, SE_a = SE_a, bias_b = bias_b, RMSE_b = RMSE_b, SE_b = SE_b,
           bias_g = bias_g, RMSE_g = RMSE_g, SE_g = SE_g)
  ret
}

res <- runSimulation(Design, replications = 100, parallel = T, packages = 'mirt',
                     generate=Generate, analyse=Analyse, summarise=Summarise, save_results=T)
```
