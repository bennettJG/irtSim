---
title: "Anchor Set Size for Concurrent Estimation of Item Response Theory Models"
author: "Ben Attaway"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
---

# Introduction
Item Response Theory (IRT) models are used extensively in the design and scoring of standardized assessments. Each item in an assessment is modeled as having its own item characteristic curve, which expresses the probability that a respondent with a given latent trait value $\theta$ will respond correctly (in the case of an assessment with correct/incorrect answers, such as an academic test) or choose a particular scale value (as in the case of a psychological assessment with Likert-scale items). The simplest item response model for dichotomous items is the one-parameter logistic (1PL) or Rasch model, which assumes that items vary only in difficulty, the value of $\theta$ at which a respondent has a 50% probability of answering correctly. (This parameter is alternatively described as "location" in some literature, since it describes a specific location on the item characteristic curve.) The two-parameter logistic model (2PL) allows items to additionally vary in discrimination, the slope of the item characteristic curve at its inflection point (de Ayala, 2009). The three-parameter logistic (3PL) model extends the 2PL model by adding a guessing parameter corresponding to the probability that a very low-$\theta$ respondent will answer correctly (the y-intercept of the item characteristic curve), which is particularly relevant for multiple-choice items. There is no consensus on the “best” model to use for large-scale assessments – although more complex models may fit real-world data most closely (see, e.g., Robitsch, 2022), ability estimates become dependent on the sample of individuals taking the assessment, which proponents of the Rasch model argue is an undesirable property (Stemler & Naples, 2021). In practice, large-scale assessments vary in the item response models used: the Programme for International Student Assessment (PISA) uses a one-parameter model (Okubo, 2022, p. 11), while the National Assessment of Educational Progress (NAEP) uses two- or three- parameter models depending on question type (NCES, n.d.-a).

In many cases, it is desirable to create multiple versions of a given assessment rather than administering the same set of items to all test-takers. In order to estimate all IRT parameters on the same scale, so that estimated ability scores are comparable across versions, it is necessary for some items to appear on multiple versions of the assessment for calibration. It is possible to first estimate parameters for each assessment version before transforming each set of parameters to a common scale (Haebara, 1980; Stocking & Lord, 1983), or to estimate all parameters simultaneously using Maximum Likelihood techniques. Separate estimation with linking is necessary in many real-world scenarios (for example, when a test is administered yearly, scores from one year must be reported before the next year’s test has been administered), but concurrent estimation tends to produce more accurate parameter estimates when it is feasible. Hanson & Béguin (2002) report obtaining parameter estimates with lower RMSE using concurrent estimation in most simulation scenarios tested. Likewise, Kim & Kolen (2007) report lower mean squared error of item characteristic functions under concurrent estimation than the Haebara or Stocking-Lord linking methods across a variety of criterion functions used for linking and underlying ability distributions.

Ideally, assessment designers would choose the lowest number of anchor items needed to obtain accurate parameter estimates, to minimize opportunities for cheating or memorization of answers from a past test version. However, reducing the anchor set too far risks non-identifiability of the IRT model. Some work on anchor set design to date has focused on test linking rather than concurrent calibration (e.g. Vale, 1986; de Gruijter, 1988; Yang & Houang, 1996), but Wingersky et al. (1987) investigated anchor sets of 10, 20, and 40 items and 85 non-anchor items under concurrent calibration (finding, unsurprisingly, that item parameter estimates based on larger estimates were more stable across simulations). García-Pérez et al. (2010) simulated varying anchor set size and criteria used for selection for a shorter test under the graded-response model, which is used for polytomous items. The current study takes a similar approach to identify appropriate anchor set sizes for dichotomous items under the 2PL model, but differs in that the total number of items per test, rather than the total item bank size, is held constant. 

# Methods
Models were fit using the `mirt` package in R (Chalmers, 2012). `mirt` implements full-information maximum likelihood methods for IRT parameters (Bock & Aiken, 1981; Bock et al., 1988) as an option for model fitting in the presence of missing data. Additionally, the `fscores` function used to produce $\theta$ estimates based on a fitted model is able to produce either point estimates (expected a-posteriori, maximum a-posteriori, or maximum likelihood) with standard errors, or a set of multiply imputed plausible values. There is theoretical justification for assessing the accuracy of both point estimates and plausible values -- the former are needed to assign scale scores to test-takers based on their responses (which may impact their future educational trajectory in the case of state testing or the SAT/ACT exams), while the latter are frequently part of publicly available datasets used to investigate group differences.

All scenarios tested were based on the 2PL model and used a total test length of 40 items and a total of 1000 test-takers. For all items, discrimination parameters $a$ were drawn from a lognormal distribution with mean 0 and $\sigma$ 0.25, and difficulty $b$ was drawn from $\mathcal{N}(0, 1.15)$. These distributions were chosen so that the majority of $a$ values would fall between 0 and 2 and the majority of $b$ values would fall between -3 and 3, noted to be typical values for NAEP (NCES, n.d.-b).

To assess the efficacy of detecting group differences based on plausible values, two conditions were used for test-taker $\theta$ distribution: One in which all 1000 individuals had values drawn from $\mathcal{N}(0, 1)$, and one in which one group of patients had $\theta ~ \mathcal{N}(0, 1)$ and the other had $\theta ~ \mathcal{N}(0.5, 1)$. The default `mirt` function used to estimate IRT parameters appears to assume an MCAR model where the group of test-takers assigned to each version are drawn from the same distribution; the `multipleGroup` function is considerably slower but can be used to fit a model in which group ability distributions differ across versions.

# Results

```{r, fig.asp=1}
library(tidyverse)
library(SimDesign)

filenames <- list.files("2PL", pattern="*.rds", full.names=TRUE)
list_results <- lapply(filenames, readRDS)
diff_Thetas <- list()
for (i in 1:length(list_results)) {
  diff_Thetas[[i]] <- do.call(cbind, lapply(1:200, function(j, res) c(res[[j]]$diff_Theta),
                   res=list_results[[i]]$results))
}

conditions <- do.call(rbind, lapply(list_results, function(i) data.frame(i$condition))) |>
  dplyr::select(n_anchor, assumption, n_test_versions, groupdiff)
biases <- lapply(diff_Thetas, bias) 
biases_long <- data.frame(matrix(unlist(biases), nrow=40, byrow=T)) |>
  cbind(conditions) |> 
  pivot_longer(starts_with("X"), names_to="iter", values_to="bias")

ggplot(biases_long |> mutate(n_anchor = factor(n_anchor)), aes(x=n_anchor, y=bias)) +
  facet_wrap(vars(assumption, groupdiff, n_test_versions)) +
  geom_jitter(alpha=0.2, height=0, color="green") +
  geom_violin(color="blue", fill="transparent") +
  stat_summary(color="blue", fun.data="mean_sdl") +
  theme_bw()

RMSEs <- lapply(diff_Thetas, RMSE) 
RMSEs <- data.frame(matrix(unlist(RMSEs), nrow=40, byrow=T)) |>
  cbind(conditions)

ggplot(RMSEs |> pivot_longer(starts_with("X"), names_to="iter", values_to="rmse_Theta") |> mutate(n_anchor = factor(n_anchor)), aes(x=n_anchor, y=rmse_Theta)) +
  facet_wrap(vars(assumption, groupdiff, n_test_versions)) +
  geom_jitter(alpha=0.2, height=0, color="green") +
  geom_violin(color="blue", fill="transparent") +
  stat_summary(color="blue", fun.data="mean_sdl") +
  theme_bw()

groupdiffs <- data.frame(matrix(nrow=40, ncol=200))
for (i in 1:length(list_results)) {
  groupdiffs[i,] <- do.call(c, lapply(1:200, function(j, res) res[[j]]$groupdiff,
                   res=list_results[[i]]$results))
}

groupdiffs <- groupdiffs |>
  cbind(conditions)

ggplot(groupdiffs |> pivot_longer(starts_with("X"), names_to="iter", values_to="diff") |> mutate(n_anchor = factor(n_anchor)), aes(x=n_anchor, y=diff)) +
  facet_wrap(vars(assumption, groupdiff, n_test_versions)) +
  geom_jitter(alpha=0.2, height=0, color="green") +
  geom_violin(color="blue", fill="transparent") +
  stat_summary(color="blue", fun.data="mean_sdl") +
  theme_bw()

theta_corrs <- data.frame(matrix(nrow=40, ncol=200))
for (i in 1:length(list_results)) {
  theta_corrs[i,] <- do.call(c, lapply(1:200, function(j, res) cor(c(res[[j]]$pred_theta[,1]), y=c(res[[j]]$true_parameters$Theta)),
                   res=list_results[[i]]$results))
}

theta_corrs <- theta_corrs |>
  cbind(conditions)

ggplot(theta_corrs |> pivot_longer(starts_with("X"), names_to="iter", values_to="corr") |> mutate(n_anchor = factor(n_anchor)), aes(x=n_anchor, y=corr)) +
  facet_wrap(vars(assumption, groupdiff, n_test_versions)) +
  geom_jitter(alpha=0.2, height=0, color="green") +
  geom_violin(color="blue", fill="transparent") +
  stat_summary(color="blue", fun.data="mean_sdl") +
  theme_bw()

diff_as <- list()
for (i in 1:length(list_results)) {
  diff_as[[i]] <- do.call(cbind, lapply(1:200, function(j, res) c(res[[j]]$diff_a[1:40]),
                   res=list_results[[i]]$results))
}

abiases <- lapply(diff_as, bias) 
abiases_long <- data.frame(matrix(unlist(abiases), nrow=40, byrow=T)) |>
  cbind(conditions) |> 
  pivot_longer(starts_with("X"), names_to="iter", values_to="bias")

ggplot(abiases_long |> mutate(n_anchor = factor(n_anchor)), aes(x=n_anchor, y=bias)) +
  facet_wrap(vars(assumption, groupdiff, n_test_versions)) +
  geom_jitter(alpha=0.2, height=0, color="green") +
  geom_violin(color="blue", fill="transparent") +
  stat_summary(color="blue", fun.data="mean_sdl") +
  theme_bw()

diff_bs <- list()
for (i in 1:length(list_results)) {
  diff_bs[[i]] <- do.call(cbind, lapply(1:200, function(j, res) c(res[[j]]$diff_b[1:40]),
                   res=list_results[[i]]$results))
}

bbiases <- lapply(diff_bs, bias) 
bbiases_long <- data.frame(matrix(unlist(bbiases), nrow=40, byrow=T)) |>
  cbind(conditions) |> 
  pivot_longer(starts_with("X"), names_to="iter", values_to="bias")

ggplot(bbiases_long |> mutate(n_anchor = factor(n_anchor)), aes(x=n_anchor, y=bias)) +
  facet_wrap(vars(assumption, groupdiff, n_test_versions)) +
  geom_jitter(alpha=0.2, height=0, color="green") +
  geom_violin(color="blue", fill="transparent") +
  stat_summary(color="blue", fun.data="mean_sdl") +
  theme_bw()

se_Thetas <- list()
for (i in 1:length(list_results)) {
  se_Thetas[[i]] <- do.call(cbind, lapply(1:200, function(j, res) c(res[[j]]$se_Theta),
                   res=list_results[[i]]$results))
}

se_Thetas <- lapply(se_Thetas, colMeans)
se_Thetas_long <- data.frame(matrix(unlist(se_Thetas), nrow=40, byrow=T)) |>
  cbind(conditions) |> 
  pivot_longer(starts_with("X"), names_to="iter", values_to="avg_se_Theta")

ggplot(se_Thetas_long |> mutate(n_anchor = factor(n_anchor)), aes(x=n_anchor, y=avg_se_Theta)) +
  facet_wrap(vars(assumption, groupdiff, n_test_versions), ncol=1, strip.position="right") +
  geom_jitter(alpha=0.2, height=0, color="green") +
  geom_violin(color="blue", fill="transparent") +
  stat_summary(color="blue", fun.data="mean_sdl") +
  theme_bw()
```

```{r, fig.asp=1.5, fig.width=6, fig.dpi=300}
list_point_thetas <- list()
for (i in 1:length(list_results)) {
  point_thetas <- data.frame(matrix(nrow=1000, ncol=5))
  names(point_thetas) <- c("pred", "pred_SE", "true_theta", "n_anchor","condition")
  results <- list_results[[i]]$results[[1]]
  point_thetas[,"pred"] <- results$pred_theta[,1]
  point_thetas[,"pred_SE"] <- results$pred_theta[,2]
  point_thetas[,"true_theta"] <- c(unlist(results$true_parameters$Thetas))
  point_thetas[,"n_anchor"] <- rep(list_results[[i]]$condition$n_anchor, 1000)
  point_thetas[,"condition"] <- rep(
    paste0(list_results[[i]]$condition$assumption, ", group diff=", list_results[[i]]$condition$groupdiff, ", test versions=", list_results[[i]]$condition$n_test_versions), 1000)
  list_point_thetas[[i]] <- point_thetas |> arrange(true_theta)
}

point_thetas_large <- bind_rows(list_point_thetas) |> 
  mutate(n_anchor = as.factor(n_anchor), condition = as.factor(condition)) |> 
  split(~condition)

for(i in 1:length(point_thetas_large)) {
  png(filename=paste0("figures/theta_point_est/", point_thetas_large[[i]]$condition, ".png"),
      width = 6, height = 9, res=72,
    units = "in", pointsize = 12, bg = "white")
    print(ggplot(point_thetas_large[[i]], aes(x=true_theta, y = pred - true_theta)) +
      labs(title=point_thetas_large[[i]]$condition) +
      facet_wrap(vars(n_anchor), ncol=1, strip.position="right", axes="all") +
      geom_errorbar(aes(ymin = pred - true_theta - pred_SE, ymax=pred - true_theta + pred_SE, x=true_theta), 
                    color="green", alpha=0.3, linewidth=1) +
      geom_point(color="darkgreen", size=1, alpha=0.5) +
      geom_abline(slope=0, intercept=0, color="blue") +
      theme_bw() +
      theme(strip.background = element_blank(),
            strip.text.y= element_text(angle = 0, color = 'blue4', size=16)))
  dev.off()
}
```

# References
Bock, R. D., & Aitkin, M. (1981). Marginal maximum likelihood estimation of item parameters: Application of an EM algorithm. *Psychometrika, 46*(4), 443-459. https://doi.org/10.1007/BF02293801

Bock, R. D., Gibbons, R., & Muraki, E. (1988). Full-information item factor analysis. *Applied Psychological Measurement, 12*(3), 261-280. https://doi.org/10.1177/014662168801200305

Chalmers, R. P. (2012). mirt: A Multidimensional Item Response Theory Package for the R Environment. Journal of Statistical Software, 48(6), 1–29. https://doi.org/10.18637/jss.v048.i06

de Ayala, R. J. (2009). The theory and practice of item response theory. New York: Guilford Press.

de Gruijter, D. N. M. (1988). Standard errors of item parameter estimates in incomplete designs. Applied Psychological Measurement, 12(2), 109–116. https://doi.org/10.1177/014662168801200201

García-Pérez, M. A., Alcalà-Quintana, R., & García-Cueto, E. (2010). A Comparison of Anchor-Item Designs for the Concurrent Calibration of Large Banks of Likert-Type Items. *Applied Psychological Measurement, 34*(8), 580–599. https://doi.org/10.1177/0146621609351259

Haebara, T. (1980). Equating logistic ability scales by a weighted least squares method. Japanese Psychological Research, 22(3), 144-149.

Hanson, B. A., & Béguin, A. A. (2002). Obtaining a common scale for Item Response Theory item parameters using separate versus concurrent estimation in the common-item equating design. Applied Psychological Measurement, 26(1), 3–24. https://doi.org/10.1177/0146621602026001001

Kim, S., & Kolen, M. J. (2007). Effects on Scale Linking of Different Definitions of Criterion Functions for the IRT Characteristic Curve Methods. Journal of Educational and Behavioral Statistics, 32(4), 371-397. https://doi.org/10.3102/1076998607302632

National Center for Education Statistics. (n.d.-a). NAEP Technical Documentation: Item Scaling Models. Retrieved November 23, 2025 from https://nces.ed.gov/nationsreportcard/tdw/analysis/scaling_models.aspx

National Center for Education Statistics. (n.d.-b). NAEP Technical Documentation: NAEP Assessment IRT Parameters. Retrieved November 23, 2025 from https://nces.ed.gov/nationsreportcard/tdw/analysis/scaling_irt.aspx

Okubo, T. (2022). Theoretical considerations on scaling methodology in PISA. OECD Education Working Papers, no. 282. https://doi.org/10.1787/c224dbeb-en

Robitzsch, A. (2022). On the choice of the item response model for scaling PISA data: Model selection based on information criteria and quantifying model uncertainty. Entropy, 24(6), 760. https://doi.org/10.3390/e24060760

Stemler, S. E. & Naples, A. (2021). Rasch measurement v. Item Response Theory: Knowing when to cross the line. Practical Assessment, Research, and Evaluation, 26(11). https://doi.org/10.7275/v2gd-4441

Stocking, M. L., & Lord, F. M. (1983). Developing a Common Metric in Item Response Theory. Applied Psychological Measurement, 7(2), 201-210. https://doi.org/10.1177/014662168300700208

Vale, C. D. (1986). Linking Item Parameters Onto a Common Scale. Applied Psychological Measurement, 10(4), 333–344. https://doi.org/10.1177/014662168601000402

Wingersky, M. S., Cook, L. L., & Eignor, D. R. (1987). Specifying the characteristics of linking items used for item response theory calibration. ETS Research Report 87 – 24.
Princeton, NJ: Educational Testing Service.

Yang, W.-L., & Houang, R. T. (1996). The Effect of Anchor Length and Equating Method on the Accuracy of Test Equating: Comparisons of Linear and IRT-Based Equating Using an Anchor-Item Design. Paper presented at the Annual Meeting of the American Educational Research Association (New York, NY, April 8-12, 1996). https://eric.ed.gov/?id=ED401308